
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow\n",
    "### Recitation Notebook\n",
    "### Authors: Trevin Gandhi, Jordan Hurwitz, Brady Neal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recitation will consist of two parts:  \n",
    "[1) Building a feedforward Deep Neural Network in TensorFlow and discussing some best practices](#section1)  \n",
    "[2) Using TensorBoard for visualizations](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#section1'></a>\n",
    "### Section 1: Building a Deep Feedforward Neural Network\n",
    "(Based on the TensorFlow tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick first thing to note --- for most applications of deep learning \n",
    "(for example, image recognition), instead of training a deep neural\n",
    "network from scratch (which can take on the order of days or weeks), it\n",
    "is common to download weights for pre-trained networks and \"fine-tune\"\n",
    "the network to fit your application. This allows you to train a neural \n",
    "network even when you don't have a bunch of data. However, the data \n",
    "that the pretrained model was trained on has to be similar \n",
    "to your data. \n",
    "\n",
    "In this notebook, however, we train the networks from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we include imports to make this\n",
    "# notebook Python 2/3 compatible.\n",
    "# You might need to pip install future\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from builtins import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we do the basic setup.\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph() # Just in case we're rerunning code in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will be training this deep neural network on MNIST,\n",
    "# so let's first load the dataset.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's initialize some placeholders\n",
    "\n",
    "# Here, x is a placeholder for our input data. Since MNIST\n",
    "# uses 28x28 pixel images, we \"unroll\" them into a 784-pixel\n",
    "# long vector. The `None` indicates that we can input an\n",
    "# arbitrary amount of datapoints. Thus we are saying x is a\n",
    "# matrix with 784 columns and an arbitrary (to be decided \n",
    "# when we supply the data) number of rows.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# We define y to be the placeholder for our *true* y's. \n",
    "# We are giving y 10 rows because each row will be a\n",
    "# one-hot vector with the correct classification of the\n",
    "# image.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we make a handy function for initializing biases. \n",
    "# Note that we are returning a \"Variable\" - this means\n",
    "# something that is subject to change during training.\n",
    "# TensorFlow is actually using gradient descent to optimize\n",
    "# the value of all \"Variables\" in our network. \n",
    "def bias_variable(shape):\n",
    "    # Here we just choose to initialize our biases to 0.\n",
    "    # However, this is not an agreed-upon standard and\n",
    "    # some initialize the biases to 0.01 to ensure\n",
    "    # that all ReLU units fire in the beginning.\n",
    "    initial = tf.constant(0.00, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define the first set of weights and biases (corresponding to our first layer)\n",
    "# We use He initialization for the weights as good practice for when we're training\n",
    "# deeper networks. Here, get_variable is similar to when we return a Variable and assign\n",
    "# it, except it also checks to see if the variable already exists.\n",
    "\n",
    "# This is: [number of input neurons, number of neurons in the first hidden layer,\n",
    "# number of neurons in the second hidden layer, number of classes]\n",
    "num_neurons = [784, 1280, 768, 10]\n",
    "\n",
    "# Just store this for convenience\n",
    "he_init  = tf.contrib.layers.variance_scaling_initializer()\n",
    "activ_fn = tf.nn.relu \n",
    "\n",
    "w1 = tf.get_variable(\"w1\", shape=[num_neurons[0], num_neurons[1]], \n",
    "                     initializer=he_init)\n",
    "b1 = bias_variable([num_neurons[1]])\n",
    "\n",
    "# Now let's define the computation that takes this layer's input and runs it through\n",
    "# the neurons. Note that we use the ReLU activation function to avoid problems\n",
    "# with our gradients. This line is the equivalent of saying the output of the\n",
    "# first hidden layer is max(x*w1 + b1, 0).\n",
    "h1 = activ_fn(tf.matmul(x, w1) + b1)\n",
    "\n",
    "# We also apply dropout after this layer and the next. Dropout is a form of regularization\n",
    "# in neural networks where we \"turn off\" randomly selected neurons during training.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the second layer, similarly to the first.\n",
    "w2 = tf.get_variable(\"w2\", shape=[num_neurons[1], num_neurons[2]], \n",
    "                     initializer=he_init)\n",
    "b2 = bias_variable([num_neurons[2]])\n",
    "h2 = activ_fn(tf.matmul(h1_drop, w2) + b2)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "# And define the third layer to output the log probabilities.\n",
    "# Note that this wouldn't really be considered a \"deep\" network\n",
    "# since there's only two hidden layers, but it should be clear to\n",
    "# see how hidden layers can easily be added at this point to make\n",
    "# it \"deep\".\n",
    "w3 = tf.get_variable(\"w3\", shape=[num_neurons[2], num_neurons[3]], \n",
    "                     initializer=he_init)\n",
    "b3 = bias_variable([num_neurons[3]])\n",
    "logits = tf.matmul(h2_drop, w3) + b3"
   ]