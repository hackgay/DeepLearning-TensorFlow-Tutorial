
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow\n",
    "### Recitation Notebook\n",
    "### Authors: Trevin Gandhi, Jordan Hurwitz, Brady Neal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recitation will consist of two parts:  \n",
    "[1) Building a feedforward Deep Neural Network in TensorFlow and discussing some best practices](#section1)  \n",
    "[2) Using TensorBoard for visualizations](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#section1'></a>\n",
    "### Section 1: Building a Deep Feedforward Neural Network\n",
    "(Based on the TensorFlow tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick first thing to note --- for most applications of deep learning \n",
    "(for example, image recognition), instead of training a deep neural\n",
    "network from scratch (which can take on the order of days or weeks), it\n",
    "is common to download weights for pre-trained networks and \"fine-tune\"\n",
    "the network to fit your application. This allows you to train a neural \n",
    "network even when you don't have a bunch of data. However, the data \n",
    "that the pretrained model was trained on has to be similar \n",
    "to your data. \n",
    "\n",
    "In this notebook, however, we train the networks from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we include imports to make this\n",
    "# notebook Python 2/3 compatible.\n",
    "# You might need to pip install future\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from builtins import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we do the basic setup.\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph() # Just in case we're rerunning code in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will be training this deep neural network on MNIST,\n",
    "# so let's first load the dataset.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's initialize some placeholders\n",
    "\n",
    "# Here, x is a placeholder for our input data. Since MNIST\n",
    "# uses 28x28 pixel images, we \"unroll\" them into a 784-pixel\n",
    "# long vector. The `None` indicates that we can input an\n",
    "# arbitrary amount of datapoints. Thus we are saying x is a\n",
    "# matrix with 784 columns and an arbitrary (to be decided \n",
    "# when we supply the data) number of rows.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# We define y to be the placeholder for our *true* y's. \n",
    "# We are giving y 10 rows because each row will be a\n",
    "# one-hot vector with the correct classification of the\n",
    "# image.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we make a handy function for initializing biases. \n",
    "# Note that we are returning a \"Variable\" - this means\n",
    "# something that is subject to change during training.\n",
    "# TensorFlow is actually using gradient descent to optimize\n",
    "# the value of all \"Variables\" in our network. \n",
    "def bias_variable(shape):\n",
    "    # Here we just choose to initialize our biases to 0.\n",
    "    # However, this is not an agreed-upon standard and\n",
    "    # some initialize the biases to 0.01 to ensure\n",
    "    # that all ReLU units fire in the beginning.\n",
    "    initial = tf.constant(0.00, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define the first set of weights and biases (corresponding to our first layer)\n",
    "# We use He initialization for the weights as good practice for when we're training\n",
    "# deeper networks. Here, get_variable is similar to when we return a Variable and assign\n",
    "# it, except it also checks to see if the variable already exists.\n",
    "\n",
    "# This is: [number of input neurons, number of neurons in the first hidden layer,\n",
    "# number of neurons in the second hidden layer, number of classes]\n",
    "num_neurons = [784, 1280, 768, 10]\n",
    "\n",
    "# Just store this for convenience\n",
    "he_init  = tf.contrib.layers.variance_scaling_initializer()\n",
    "activ_fn = tf.nn.relu \n",
    "\n",
    "w1 = tf.get_variable(\"w1\", shape=[num_neurons[0], num_neurons[1]], \n",
    "                     initializer=he_init)\n",
    "b1 = bias_variable([num_neurons[1]])\n",
    "\n",
    "# Now let's define the computation that takes this layer's input and runs it through\n",
    "# the neurons. Note that we use the ReLU activation function to avoid problems\n",
    "# with our gradients. This line is the equivalent of saying the output of the\n",
    "# first hidden layer is max(x*w1 + b1, 0).\n",
    "h1 = activ_fn(tf.matmul(x, w1) + b1)\n",
    "\n",
    "# We also apply dropout after this layer and the next. Dropout is a form of regularization\n",
    "# in neural networks where we \"turn off\" randomly selected neurons during training.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the second layer, similarly to the first.\n",
    "w2 = tf.get_variable(\"w2\", shape=[num_neurons[1], num_neurons[2]], \n",
    "                     initializer=he_init)\n",
    "b2 = bias_variable([num_neurons[2]])\n",
    "h2 = activ_fn(tf.matmul(h1_drop, w2) + b2)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "# And define the third layer to output the log probabilities.\n",
    "# Note that this wouldn't really be considered a \"deep\" network\n",
    "# since there's only two hidden layers, but it should be clear to\n",
    "# see how hidden layers can easily be added at this point to make\n",
    "# it \"deep\".\n",
    "w3 = tf.get_variable(\"w3\", shape=[num_neurons[2], num_neurons[3]], \n",
    "                     initializer=he_init)\n",
    "b3 = bias_variable([num_neurons[3]])\n",
    "logits = tf.matmul(h2_drop, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We define our loss function to be cross entropy over softmax probabilities.\n",
    "# Here our true labels are defined by y, and our log probabilities\n",
    "# (TensorFlow calls them `logits`) are defined by logits.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "# If we wanted, we could also add L2 weight regularization by adding\n",
    "# the following lines to the loss function\n",
    "#     0.0001*tf.nn.l2_loss(w1) +\\\n",
    "#     0.0001*tf.nn.l2_loss(w2) +\\\n",
    "#     0.0001*tf.nn.l2_loss(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will use the `Adam` optimizer. Adam is an fancier variant of\n",
    "# standard gradient descent.\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Here we build a binary vector corresponding to where our predicted \n",
    "# classes matched the actual classes.\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    num_train = mnist.train.num_examples\n",
    "    num_test  = mnist.test.num_examples\n",
    "\n",
    "    num_epochs = 20\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_epochs):\n",
    "        for _ in range(num_train / batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        # Get an estimate of our current progress using the last batch\n",
    "        train_accuracy, loss = sess.run([accuracy, cross_entropy], \n",
    "                                    feed_dict={x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        print(\"epoch %d completed: training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "\n",
    "    # Test\n",
    "    test_accuracy = 0\n",
    "    for _ in range(num_test / batch_size):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        test_accuracy += batch_size * accuracy.eval(feed_dict={\n",
    "            x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "\n",
    "    print(\"test accuracy %g\"%(test_accuracy / num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we make this simpler? \n",
    "With TensorFlow 1.0, we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, dropout, batch_norm\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x  = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "num_neurons = [784, 1280, 768, 10]\n",
    "he_init  = tf.contrib.layers.variance_scaling_initializer()\n",
    "activ_fn = tf.nn.relu \n",
    "\n",
    "# Instead of making keep_prob a placeholder (like we did for dropout\n",
    "# above), we can make a boolean `is_training` placeholder that dropout\n",
    "# and batch normalization can check to determine what parameter\n",
    "# values to use (i.e. if is_training = True, then dropout will use\n",
    "# a keep_prob of 0.5. Otherwise, it uses a keep_prob of 1.0).\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "# We can even easily add Batch Normalization, which can also be quite\n",
    "# useful when training deep neural networks (although it won't do much\n",
    "# here).\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "}\n",
    "\n",
    "# Define the first hidden layer using `fully_connected`\n",
    "# There are similar functions (e.g. conv2d) for other\n",
    "# types of layers.\n",
    "keep_prob = 0.5\n",
    "hidden1 = fully_connected(x, num_neurons[1], \n",
    "                          weights_initializer=he_init,\n",
    "                          activation_fn=activ_fn,\n",
    "                          normalizer_fn=batch_norm, \n",
    "                          normalizer_params=bn_params)\n",
    "hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "\n",
    "hidden2 = fully_connected(hidden1_drop, num_neurons[2], \n",
    "                          weights_initializer=he_init,\n",
    "                          activation_fn=activ_fn,\n",
    "                          normalizer_fn=batch_norm, \n",
    "                          normalizer_params=bn_params)\n",
    "hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "\n",
    "logits = fully_connected(hidden2_drop, num_neurons[3], activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's train it and see how it does! It should be pretty similar\n",
    "# to our previous results.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    num_train = mnist.train.num_examples\n",
    "    num_test  = mnist.test.num_examples\n",
    "\n",
    "    num_epochs = 20\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_epochs):\n",
    "        for _ in range(num_train / batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], is_training: True})\n",
    "        # Get an estimate of our current progress using the last batch\n",
    "        train_accuracy, loss = sess.run([accuracy, cross_entropy], \n",
    "                                    feed_dict={x:batch[0], y: batch[1], is_training: False})\n",
    "        print(\"epoch %d completed: training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "\n",
    "    # Test\n",
    "    test_accuracy = 0\n",
    "    for _ in range(num_test / batch_size):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        test_accuracy += batch_size * accuracy.eval(feed_dict={\n",
    "            x:batch[0], y: batch[1], is_training: False})\n",
    "\n",
    "    print(\"test accuracy %g\"%(test_accuracy / num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#section2'></a>\n",
    "### Using TensorBoard for Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See DeepLearningTensorFlowRecitation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Classification with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we will train another classifier for the MNIST dataset\n",
    "# except this time, we will use an RNN. While this may not be\n",
    "# an especially intuitive application, in my opinion, it is an\n",
    "# interesting (although also not very practical) application of\n",
    "# RNNs for that reason. \n",
    "\n",
    "# So how do we do this? Since the images are 28 x 28 pixels, we\n",
    "# will model them as a sequence of 28 pixel vectors across 28 \n",
    "# timesteps. We will feed each of these pixel vectors into a\n",
    "# GRU cell with 150 neurons. At the end of the 28 timesteps, \n",
    "# we will take the state of the RNN and feed it into a fully\n",
    "# connected layer with 10 outputs, allowing us to generate\n",
    "# log probabilities for each of the classes. Then, the rest\n",
    "# proceeds as above where we can do softmax and cross-entropy\n",
    "# on the log probabilities to determine the loss, and use that\n",
    "# loss for backpropagation through the network.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_timesteps = 28\n",
    "# [num inputs per timestep, num neurons in RNN Cell, num outputs for fully connected layer]\n",
    "num_neurons = [28, 150, 10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since the input data initially comes as a 784-dimension vector,\n",
    "# we need to reshape it back into a 28x28 image. Now x is a tensor\n",
    "# where the first dimension indexes each image.\n",
    "x = tf.placeholder(tf.float32, [None, num_timesteps, num_neurons[0]])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Here is where we define the core of the network. Right now, \n",
    "# we are using a GRU cell with 150 neurons. While a basic RNN cell\n",
    "# also works, using the GRU allows us to track long-term\n",
    "# dependencies, which improves our accuracy here. We will then feed\n",
    "# it into the dynamic_rnn function, which will run all the \n",
    "# timesteps for the RNN. Note that since we know the number of \n",
    "# timesteps for every input, we could use the static_rnn function.\n",
    "# However, the dynamic_rnn function seems to be strictly better as\n",
    "# it has an easier API (don't need to stack and unstack the data)\n",
    "# and can even support offloading GPU memory to CPU memory to avoid\n",
    "# OutOfMemory errors.\n",
    "basic_cell = tf.contrib.rnn.GRUCell(num_units=num_neurons[1])\n",
    "outputs, final_state = tf.nn.dynamic_rnn(basic_cell, x, dtype=tf.float32)\n",
    "\n",
    "# Now we take the final state of the RNN and feed it into a\n",
    "# fully connected layer to obtain our log probabilities.\n",
    "logits = fully_connected(final_state, num_neurons[2], activation_fn=None)\n",
    "\n",
    "# From here on, this code should seem familiar as it is essentially\n",
    "# the same code as above.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    num_train = mnist.train.num_examples\n",
    "\n",
    "    num_epochs = 100\n",
    "    batch_size = 150\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Train\n",
    "    for i in range(num_epochs):\n",
    "        for _ in range(num_train / batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            x_batch = batch[0].reshape((-1, num_timesteps, num_neurons[0]))\n",
    "            sess.run(train_step, feed_dict={x: x_batch, y: batch[1]})\n",
    "        train_accuracy, loss = sess.run([accuracy, cross_entropy], \n",
    "                                        feed_dict={x: x_batch, y: batch[1]})\n",
    "        print(\"epoch %d completed: training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "        \n",
    "    # Test\n",
    "    x_test = mnist.test.images.reshape((-1, num_timesteps, num_neurons[0]))\n",
    "    y_test = mnist.test.labels\n",
    "    test_accuracy = accuracy.eval(feed_dict={x: x_test, y: y_test})\n",
    "    print(\"test accuracy %g\"%(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with RNNs\n",
    "Based off the TensorFlow Tutorial and Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us first download the dataset we will be using,\n",
    "# the works of Shakespeare. Dataset from Andrej Karpathy.\n",
    "import urllib2\n",
    "print ('Downloading Shakespeare data')\n",
    "source = urllib2.urlopen(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "shakespeare = source.read()\n",
    "print ('Download complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we need to generate a mapping between unique\n",
    "# characters \n",
    "num_chars = len(set(shakespeare))\n",
    "i2c_map = {i: c for i, c in enumerate(set(shakespeare))}\n",
    "c2i_map = {c: i for i, c in i2c_map.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_timesteps = 30\n",
    "\n",
    "# [num inputs per timestep, num neurons in RNN Cell, num outputs for fully connected layer]\n",
    "num_neurons = 150 # [num_chars, 150, num_chars] \n",
    "batch_size  = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, None, num_chars])\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_chars])\n",
    "\n",
    "state = tf.zeros((batch_size, num_neurons))\n",
    "basic_cell = tf.contrib.rnn.GRUCell(num_units=num_neurons)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(basic_cell, x, dtype=tf.float32, initial_state=state)\n",
    "\n",
    "# outputs :: [batch_size, timesteps, 150]\n",